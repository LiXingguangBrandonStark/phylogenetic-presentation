### Markov Chain

A Markov chain is a type of Markov process in which the next value of a state depends only on the present state of the system.

They are of two types, discrete and continuous. 